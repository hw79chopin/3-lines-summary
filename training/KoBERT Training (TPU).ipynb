{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ohmybert_last(TPU) (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/passionyang16/context_summarize/blob/main/%E1%84%80%E1%85%A9%E1%84%82%E1%85%A1%E1%86%AB%E1%84%80%E1%85%AA%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%80%E1%85%A7%E1%86%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acJ12YgDPLZw",
        "outputId": "9b5fe298-f04d-4ec2-c774-523392901c42"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cloud-tpu-client==0.10\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Collecting torch-xla==1.7\n",
            "\u001b[?25l  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl (133.6MB)\n",
            "\u001b[K     |████████████████████████████████| 133.6MB 49kB/s \n",
            "\u001b[?25hCollecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.16.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.17.2)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.52.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (51.0.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.12.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client, torch-xla\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzB1yTdJKrUf",
        "outputId": "9a1f4762-96a5-4898-f5d7-f9054de18691"
      },
      "source": [
        "import torch_xla\r\n",
        "import torch_xla.core.xla_model as xm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.7...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.7...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.7\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG6euZksQvKd"
      },
      "source": [
        "device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2IfpzVk9A0Y"
      },
      "source": [
        "## 구글 마운트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_T7pcFk6Uow",
        "outputId": "78cc74f0-f81e-420a-c5c9-9d2fe82970ef"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZdErFiWfp7C",
        "outputId": "da245d06-0c20-46a9-b29c-c47689923500"
      },
      "source": [
        "cd gdrive/My Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MwGsk106d3J",
        "outputId": "dc43ae29-d92b-40aa-dd9d-04cbcca394e7"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "folder = \"context_summarize\" ## 자기 드라이브 경로 입력\n",
        "\n",
        "base_path = Path(\"세줄이/\")\n",
        "project_path = base_path / folder\n",
        "os.chdir(project_path)\n",
        "for x in list(project_path.glob(\"*\")):\n",
        "    if x.is_dir():\n",
        "        dir_name = str(x.relative_to(project_path))\n",
        "        os.rename(dir_name, dir_name.split(\" \", 1)[0])\n",
        "print(f\"{os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/세줄이/context_summarize\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxF2tus29GgB"
      },
      "source": [
        "## KoBERT 및 기타 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QH2Q9QX7ZO0",
        "outputId": "cfaf6dc9-5045-4464-98dc-67f1b5f3807e"
      },
      "source": [
        "!git clone https://github.com/SKTBrain/KoBERT.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'KoBERT' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKERnI39qjIF",
        "outputId": "444c54bf-cee8-447f-9f4f-2594fa2636c6"
      },
      "source": [
        "cd KoBERT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/세줄이/context_summarize/KoBERT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZbuo2OaGWft",
        "outputId": "1e089fd6-a080-419a-e465-49bf9e5a9029"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mimgs\u001b[0m/  \u001b[01;34mkobert\u001b[0m/  LICENSE  \u001b[01;34mlogs\u001b[0m/  README.md  requirements.txt  \u001b[01;34mscripts\u001b[0m/  setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdUq3wH-rNT-",
        "outputId": "80c80436-41a6-4464-964a-171a71c44582"
      },
      "source": [
        "!pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/gdrive/My Drive/세줄이/context_summarize/KoBERT\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.1.2-cp36-none-any.whl size=12734 sha256=04d00fefcb89a22e4b802f75e04b2bc4238e48892c08ee61306c27f64e18778c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d0csrol7/wheels/6b/63/d5/70ce1c6dd21f92c6e746b9ea63798e7c0e3dce4148f2adb7f0\n",
            "Successfully built kobert\n",
            "Installing collected packages: kobert\n",
            "Successfully installed kobert-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QDNPRHl87nb4",
        "outputId": "76f81568-8594-4ec5-94b3-08cbc1039787"
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install mxnet-cu101\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install transformers==2.1.1\n",
        "!pip install torch==1.3.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.7.0+cu101)\n",
            "Collecting mxnet>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/bb/54cbabe428351c06d10903c658878d29ee7026efbe45133fd133598d6eb6/mxnet-1.7.0.post1-py2.py3-none-manylinux2014_x86_64.whl (55.0MB)\n",
            "\u001b[K     |████████████████████████████████| 55.0MB 2.1MB/s \n",
            "\u001b[?25hCollecting gluonnlp>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 60.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 54.2MB/s \n",
            "\u001b[?25hCollecting onnxruntime>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/f009251fd1b91a2e1ce6f22d4b5be9936fbd0072842c5087a2a49706c509/onnxruntime-1.6.0-cp36-cp36m-manylinux2014_x86_64.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 42.4MB/s \n",
            "\u001b[?25hCollecting transformers>=3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 61.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (1.19.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (0.8)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet>=1.4.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.6.0->-r requirements.txt (line 3)) (0.29.21)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.6.0->-r requirements.txt (line 3)) (20.8)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime>=0.3.0->-r requirements.txt (line 5)) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->-r requirements.txt (line 6)) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->-r requirements.txt (line 6)) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 63.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->-r requirements.txt (line 6)) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 61.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.6.0->-r requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime>=0.3.0->-r requirements.txt (line 5)) (51.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime>=0.3.0->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.5.0->-r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.5.0->-r requirements.txt (line 6)) (1.0.0)\n",
            "Building wheels for collected packages: gluonnlp, sacremoses\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=588516 sha256=07524bd2d7894133975ad895044b1aff6846cdad638f5b6c4b2d6e66e3863251\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=f6de69133172a6ccc1124f43d60103e501fb143e0f0bdaa6f7c428ad90f419bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built gluonnlp sacremoses\n",
            "Installing collected packages: graphviz, mxnet, gluonnlp, sentencepiece, onnxruntime, sacremoses, tokenizers, transformers\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed gluonnlp-0.10.0 graphviz-0.8.4 mxnet-1.7.0.post1 onnxruntime-1.6.0 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.4 transformers-4.1.1\n",
            "Collecting mxnet-cu101\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/26/9655677b901537f367c3c473376e4106abc72e01a8fc25b1cb6ed9c37e8c/mxnet_cu101-1.7.0-py2.py3-none-manylinux2014_x86_64.whl (846.0MB)\n",
            "\u001b[K     |███████████████████████████████▌| 834.1MB 1.2MB/s eta 0:00:11tcmalloc: large alloc 1147494400 bytes == 0x39c7e000 @  0x7f8104420615 0x591e47 0x4cc179 0x4cc2db 0x50a1cc 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x58e683 0x50c127 0x58e683 0x50c127 0x58e683 0x50c127 0x58e683 0x50c127 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "\u001b[K     |████████████████████████████████| 846.0MB 19kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (2.23.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (0.8.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (1.19.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.10)\n",
            "Installing collected packages: mxnet-cu101\n",
            "Successfully installed mxnet-cu101-1.7.0\n",
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (0.29.21)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (1.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (20.8)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Collecting transformers==2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (0.1.94)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (1.19.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2.23.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/c6/912cc2cfd1b4051621552fc5961c25e2f517a090d179a38f62d5cdaf5d37/boto3-1.16.43-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 17.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2.10)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.43\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/6c/9f6e6a14f53b21b6f1670ccd789082015911458823914b7dabca333ae033/botocore-1.19.43-py2.py3-none-any.whl (7.2MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 11.1MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.43->boto3->transformers==2.1.1) (2.8.1)\n",
            "\u001b[31mERROR: botocore 1.19.43 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, transformers\n",
            "  Found existing installation: transformers 4.1.1\n",
            "    Uninstalling transformers-4.1.1:\n",
            "      Successfully uninstalled transformers-4.1.1\n",
            "Successfully installed boto3-1.16.43 botocore-1.19.43 jmespath-0.10.0 s3transfer-0.3.3 transformers-2.1.1\n",
            "Collecting torch==1.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/95/90e8c4c31cfc67248bf944ba42029295b77159982f532c5689bcfe4e9108/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (734.6MB)\n",
            "\u001b[K     |████████████████████████████████| 734.6MB 22kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.19.4)\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.3.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "Successfully installed torch-1.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2fB2BZgr99O",
        "outputId": "4afb9199-e88b-446d-cb7b-8bebd9fc77d2"
      },
      "source": [
        "cd KoBERT/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'KoBERT/'\n",
            "/content/gdrive/My Drive/세줄이/context_summarize/KoBERT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUnXyMEb823P"
      },
      "source": [
        "## 패키지 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LCtyc6C7yE9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#Sentence Tokenizer\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "\n",
        "\n",
        "# KoBERT\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "from kobert.utils import get_tokenizer\n",
        "\n",
        "# etc\n",
        "import gluonnlp as nlp\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import WarmupLinearSchedule"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkeVNrww-fxm",
        "outputId": "b7049171-e4ed-402f-f9ba-df7d56a5b1ac"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/세줄이/context_summarize\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE4khGsQ-IVn"
      },
      "source": [
        "## Colab GPU 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHfblegi91vh",
        "outputId": "98441fd5-a7f3-4d37-8652-325fdf5152a9"
      },
      "source": [
        "#device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xla:1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj0mzw-NxUMH"
      },
      "source": [
        "## KoBERT Tokenizer 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zXfZuWV94_3",
        "outputId": "415999a8-75ae-422c-8611-997a55c6a6e3"
      },
      "source": [
        "# KoBERT pretrained model 다운로드 후 모델 가져오기\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKyLPyxt-OjZ",
        "outputId": "59899773-5357-4ead-96ed-190c75e6ef69"
      },
      "source": [
        "# KoBERT pretrained tokenizer 다운로드 후 tokenizer 생성\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-G7upVm2JPs",
        "outputId": "47c260b5-6f65-4cdb-9cb3-0412be13598a"
      },
      "source": [
        "# 특수 토큰\n",
        "for k in range(10):\n",
        "  print(tok.vocab.idx_to_token[k])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[UNK]\n",
            "[PAD]\n",
            "[CLS]\n",
            "[SEP]\n",
            "[MASK]\n",
            "!\n",
            "!'\n",
            "!”\n",
            "\"\n",
            "#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SoanRDU6qCA"
      },
      "source": [
        " [UNK] : unknown token\n",
        "\n",
        " [PAD] : The token used for padding, for example when batching sequences of different lengths.\n",
        "\n",
        " [CLS] : The classifier token which is used when doing sequence classification (classification of the whole sequence instead of per-token classification).\n",
        "         It is the first token of the sequence when built with special tokens.\n",
        "\n",
        " [SEP] : The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for sequence classification or for a text\n",
        "         and a question for question answering. It is also used as the last token of a sequence built with special tokens.\n",
        "         \n",
        " [MASK]: The token used for masking values. This is the token used when training this model with masked language modeling.\n",
        "         This is the token which the model will try to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG2fMEeuzY6C"
      },
      "source": [
        "## 하이퍼파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SphBps11x5bC"
      },
      "source": [
        "max_sentence_num = 64\n",
        "max_word_num = 64\n",
        "num_workers = 5\n",
        "batch_size = 64\n",
        "learning_rate = 1e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBr49Xu-_T6w"
      },
      "source": [
        "## Dataset 클래스"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqUmOap4usvq"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "\r\n",
        "class ArticleDataset(Dataset):\r\n",
        "  \"\"\"\r\n",
        "  기사 데이터 로드\r\n",
        "\r\n",
        "  input :\r\n",
        "      dataframe : 기사 데이터\r\n",
        "      article_col : \r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  def __init__(self, dataframe, article_col, ext_col, tokenizer, max_sentence_num=50, max_word_num=100, pad=True, pair=False):\r\n",
        "\r\n",
        "    # data\r\n",
        "    self.articles = []\r\n",
        "\r\n",
        "    transform = nlp.data.BERTSentenceTransform(tokenizer, max_seq_length=max_word_num, pad=pad, pair=pair)\r\n",
        "    total_words_tokenized = []\r\n",
        "    \r\n",
        "    for article in tqdm(dataframe[article_col]):\r\n",
        "      size = max_sentence_num * max_word_num\r\n",
        "      numbers_tokenized = np.array([])\r\n",
        "\r\n",
        "      for sentence in article:\r\n",
        "        # tokenizing\r\n",
        "        tokenized_to_num = transform([sentence])\r\n",
        "        numbers_tokenized = np.concatenate((numbers_tokenized, tokenized_to_num[0]), axis = 0)\r\n",
        "      \r\n",
        "      # padding\r\n",
        "      if len(numbers_tokenized) < size:\r\n",
        "        remaining_adds = size - len(numbers_tokenized)\r\n",
        "        zeros_to_add = np.ones((remaining_adds,),dtype=np.int32)\r\n",
        "        numbers_tokenized = np.concatenate((numbers_tokenized,zeros_to_add),axis=0)\r\n",
        "      else:\r\n",
        "        numbers_tokenized = numbers_tokenized[:size]\r\n",
        "      \r\n",
        "      numbers_tokenized = numbers_tokenized.astype(np.int32)\r\n",
        "      self.articles.append(numbers_tokenized)\r\n",
        "\r\n",
        "    # label\r\n",
        "    self.summary = []\r\n",
        "    for extractive in dataframe[ext_col]:\r\n",
        "      binary = self.binary_target_processor(extractive)\r\n",
        "      self.summary.append(binary)\r\n",
        "\r\n",
        "  def binary_target_processor(self, extractive):\r\n",
        "    binary = np.zeros((max_sentence_num,1),dtype=np.int32)\r\n",
        "    for sentence_index in extractive:\r\n",
        "      if sentence_index < max_sentence_num:\r\n",
        "        binary[sentence_index] = 1\r\n",
        "    return binary\r\n",
        "\r\n",
        "  def __getitem__(self, i):\r\n",
        "    return (self.articles[i], self.summary[i])\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return len(self.articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyKgfGNC8uoX"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "\r\n",
        "class ArticleLineDataset(Dataset):\r\n",
        "  \"\"\"\r\n",
        "  기사 데이터 로드\r\n",
        "\r\n",
        "  input :\r\n",
        "      dataframe : 기사 데이터\r\n",
        "      article_col : \r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  def __init__(self, dataframe, article_col, ext_col, tokenizer, max_sentence_num=128, max_word_num=128, pad=True, pair=False):\r\n",
        "    transform = nlp.data.BERTSentenceTransform(tokenizer, max_seq_length=max_word_num, pad=pad, pair=pair)    \r\n",
        "\r\n",
        "    # data\r\n",
        "    self.articles = []\r\n",
        "    self.sentence_length = []\r\n",
        "    for article in tqdm(dataframe[article_col]):\r\n",
        "      numbers_tokenized = np.array([[]])\r\n",
        "\r\n",
        "      for sentence in article:\r\n",
        "        # tokenizing\r\n",
        "        tokenized_to_num = transform([sentence])\r\n",
        "        tokenized_to_num = np.expand_dims(tokenized_to_num[0], axis=0)\r\n",
        "        if numbers_tokenized.shape == (1, 0):\r\n",
        "          numbers_tokenized = np.concatenate((tokenized_to_num, ), axis = 0)\r\n",
        "        else:\r\n",
        "          numbers_tokenized = np.concatenate((numbers_tokenized, tokenized_to_num), axis = 0)\r\n",
        "\r\n",
        "      self.sentence_length.append(np.int32(len(numbers_tokenized)))\r\n",
        "\r\n",
        "      # padding\r\n",
        "      if len(numbers_tokenized) < max_sentence_num:\r\n",
        "        remaining_adds = max_sentence_num - len(numbers_tokenized)\r\n",
        "        padding_sentence = np.ones((remaining_adds, max_word_num),dtype=np.int32)\r\n",
        "        numbers_tokenized = np.concatenate((numbers_tokenized, padding_sentence),axis=0)\r\n",
        "      else:\r\n",
        "        numbers_tokenized = numbers_tokenized[:max_sentence_num]\r\n",
        "      \r\n",
        "      numbers_tokenized = numbers_tokenized.astype(np.int32)\r\n",
        "      self.articles.append(numbers_tokenized)\r\n",
        "\r\n",
        "    # label\r\n",
        "    self.summary = []\r\n",
        "    for extractive in dataframe[ext_col]:\r\n",
        "      binary = self.binary_target_processor(extractive)\r\n",
        "      self.summary.append(binary)\r\n",
        "\r\n",
        "  def binary_target_processor(self, extractive):\r\n",
        "    binary = np.zeros((max_sentence_num,1),dtype=np.int32)\r\n",
        "    for sentence_index in extractive:\r\n",
        "      if sentence_index < max_sentence_num:\r\n",
        "        binary[sentence_index] = 1\r\n",
        "    return binary\r\n",
        "\r\n",
        "  def __getitem__(self, i):\r\n",
        "    return (self.articles[i], self.sentence_length[i], self.summary[i])\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return len(self.articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MpNyqgXxY7L"
      },
      "source": [
        "## 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8axnV7HfuqA",
        "outputId": "b39b9c94-c8b8-4b60-d2e5-95fa8a65ea4b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KoBERT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1CqUiqQfvk-",
        "outputId": "3fb18260-0103-441b-c590-655d569c6193"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/세줄이\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDQal_1Juiyz"
      },
      "source": [
        "df = pd.read_json(\"data/train.jsonl\", lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5_K_uLNnkZH",
        "outputId": "f3f18941-c75d-4e61-a086-2037a5eb0b7c"
      },
      "source": [
        "# train_dataset = ArticleDataset(df, \"article_original\", \"extractive\", tok, max_sentence_num, max_word_num, True, False)\r\n",
        "train_dataset = ArticleLineDataset(df, \"article_original\", \"extractive\", tok, max_sentence_num, max_word_num, True, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 42803/42803 [01:46<00:00, 400.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQlewstp-6rr"
      },
      "source": [
        "class MyDataLoader():\r\n",
        "  def __init__(self, dataset, batch_size):\r\n",
        "    self.current = 0\r\n",
        "    self.dataset = [item for item in dataset]\r\n",
        "    self.batch_size = batch_size\r\n",
        "    if len(dataset) % batch_size == 0:\r\n",
        "      self.stop = len(dataset) // batch_size\r\n",
        "    else: \r\n",
        "      self.stop = len(dataset) // batch_size + 1\r\n",
        "    self.batch = self.devide_batch()\r\n",
        "\r\n",
        "  def devide_batch(self):\r\n",
        "    import random\r\n",
        "\r\n",
        "    random.shuffle(self.dataset)\r\n",
        "\r\n",
        "    batchs = [ [] for _ in range(self.stop) ]\r\n",
        "\r\n",
        "    for i, dt in enumerate(self.dataset):\r\n",
        "      batchs[i % self.stop].append((torch.from_numpy(dt[0]).unsqueeze(0), torch.tensor(dt[1]).unsqueeze(0), torch.from_numpy(dt[2]).unsqueeze(0)))\r\n",
        "    \r\n",
        "    return batchs\r\n",
        "\r\n",
        "  def __iter__(self):\r\n",
        "    return self\r\n",
        "\r\n",
        "  def __next__(self):\r\n",
        "    if self.current < self.stop:    # 현재 숫자가 반복을 끝낼 숫자보다 작을 때\r\n",
        "      flag = True\r\n",
        "      for article, num, ext in self.batch[self.current]:\r\n",
        "        if flag:\r\n",
        "          articles = torch.cat([article, ], dim=0)\r\n",
        "          nums = torch.cat([num, ], dim=0)\r\n",
        "          exts = torch.cat([ext, ], dim=0)\r\n",
        "          flag = False\r\n",
        "        else:\r\n",
        "          articles = torch.cat([articles, article], dim=0)\r\n",
        "          nums = torch.cat([nums, num], dim=0)\r\n",
        "          exts = torch.cat([exts, ext], dim=0)\r\n",
        "\r\n",
        "      self.current += 1           # 현재 숫자를 1 증가시킴\r\n",
        "      return [articles, nums, exts]\r\n",
        "    else:                           # 현재 숫자가 반복을 끝낼 숫자보다 크거나 같을 때\r\n",
        "      self.current = 0\r\n",
        "      random.shuffle(self.dataset)\r\n",
        "      raise StopIteration\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpavLInKPRRp"
      },
      "source": [
        "loader = MyDataLoader(train_dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1xKhpMlcmvR"
      },
      "source": [
        "## 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-PANATK6Hd_"
      },
      "source": [
        "#positional encoding function\r\n",
        "def get_sinusoid_encoding_table(n_seq, d_hidn):\r\n",
        "    def cal_angle(position, i_hidn):\r\n",
        "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\r\n",
        "    def get_posi_angle_vec(position):\r\n",
        "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\r\n",
        "\r\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\r\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \r\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\r\n",
        "\r\n",
        "    return sinusoid_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HYTaYcv6Hwu"
      },
      "source": [
        "#Mask padding function\r\n",
        "def get_attn_pad_mask(seq_q, seq_k, i_pad):\r\n",
        "    batch_size, len_q = seq_q.size()\r\n",
        "    batch_size, len_k = seq_k.size()\r\n",
        "    pad_attn_mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)\r\n",
        "    return pad_attn_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LImwbFF6H2q"
      },
      "source": [
        "# ScaledDotProductAttention Class\r\n",
        "class ScaledDotProductAttention(nn.Module):\r\n",
        "  def __init__(self, d_head):\r\n",
        "    super().__init__()\r\n",
        "    self.scale = 1 / (d_head **0.5)\r\n",
        "  def forward(self, Q, K, V, attn_mask):\r\n",
        "    scores = torch.matmul(Q, K.transpose(-1,-2)).mul_(self.scale)\r\n",
        "    scores.masked_fill_(attn_mask, -1e9)\r\n",
        "    attn_prob = nn.Softmax(dim=-1)(scores)\r\n",
        "    context = torch.matmul(attn_prob, V)\r\n",
        "    return context, attn_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imo8t-af6H-H"
      },
      "source": [
        "# MultiHeadAttention Class\r\n",
        "class MultiHeadAttention(nn.Module):\r\n",
        "    def __init__(self, d_hidn, n_head, d_head):\r\n",
        "        super().__init__()\r\n",
        "        self.d_hidn = d_hidn\r\n",
        "        self.n_head = n_head\r\n",
        "        self.d_head = d_head\r\n",
        "\r\n",
        "        self.W_Q = nn.Linear(d_hidn, n_head * d_head)\r\n",
        "        self.W_K = nn.Linear(d_hidn, n_head * d_head)\r\n",
        "        self.W_V = nn.Linear(d_hidn, n_head * d_head)\r\n",
        "        self.scaled_dot_attn = ScaledDotProductAttention(d_head)\r\n",
        "        self.linear = nn.Linear(n_head * d_head, d_hidn)\r\n",
        "    \r\n",
        "    def forward(self, Q, K, V, attn_mask):\r\n",
        "        batch_size = Q.size(0)\r\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2)\r\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2)\r\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2)\r\n",
        "\r\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_head, 1, 1)\r\n",
        "\r\n",
        "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\r\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_head * self.d_head)\r\n",
        "        output = self.linear(context)\r\n",
        "\r\n",
        "        return output, attn_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KLwuKiV6IEC"
      },
      "source": [
        "#PoswiseFeedForwardNet Class\r\n",
        "class PoswiseFeedForwardNet(nn.Module):\r\n",
        "  def __init__(self, d_hidn):\r\n",
        "    super().__init__()\r\n",
        "    self.d_hidn = d_hidn\r\n",
        "\r\n",
        "    self.conv1 = nn.Conv1d(in_channels=d_hidn, out_channels=d_hidn*4, kernel_size=1)\r\n",
        "    self.conv2 = nn.Conv1d(in_channels=d_hidn*4, out_channels=d_hidn, kernel_size=1)\r\n",
        "    self.active = F.gelu\r\n",
        "\r\n",
        "  def forward(self, inputs):\r\n",
        "    output = self.active(self.conv1(inputs.transpose(1,2)))\r\n",
        "    output = self.conv2(output).transpose(1,2)\r\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6FQoVmD6QuM"
      },
      "source": [
        "\"\"\" encoder layer \"\"\"\r\n",
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        self.self_attn = MultiHeadAttention(self.config.d_hidn, self.config.n_head, self.config.d_head)\r\n",
        "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\r\n",
        "        self.pos_ffn = PoswiseFeedForwardNet(self.config.d_hidn)\r\n",
        "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\r\n",
        "    \r\n",
        "    def forward(self, inputs, attn_mask):\r\n",
        "        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\r\n",
        "        att_outputs = self.layer_norm1(inputs + att_outputs)\r\n",
        "        ffn_outputs = self.pos_ffn(att_outputs)\r\n",
        "        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)\r\n",
        "        return ffn_outputs, attn_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVZNOyEB6Q0R"
      },
      "source": [
        "\r\n",
        "\"\"\" encoder \"\"\"\r\n",
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, config, n_layer):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\r\n",
        "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_enc_seq + 1, self.config.d_hidn))\r\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\r\n",
        "        self.n_layer = n_layer\r\n",
        "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.n_layer)])\r\n",
        "    \r\n",
        "    def forward(self, inputs):\r\n",
        "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\r\n",
        "        pos_mask = inputs.eq(self.config.i_pad)\r\n",
        "        positions.masked_fill_(pos_mask, 0)\r\n",
        "\r\n",
        "        outputs = self.enc_emb(inputs) + self.pos_emb(positions)\r\n",
        "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\r\n",
        "\r\n",
        "        attn_probs = []\r\n",
        "        for layer in self.layers:\r\n",
        "            outputs, attn_prob = layer(outputs, attn_mask)\r\n",
        "            attn_probs.append(attn_prob)\r\n",
        "        return outputs, attn_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoZrW_106Q9L"
      },
      "source": [
        "\"\"\" encoder \"\"\"\r\n",
        "class SecondEncoder(Encoder):\r\n",
        "    def __init__(self, config, n_layer):\r\n",
        "        super().__init__(config, n_layer)\r\n",
        "\r\n",
        "    \r\n",
        "    def forward(self, inputs, cls_inputs):\r\n",
        "        positions = torch.arange(cls_inputs.size(1), device=device, dtype=cls_inputs.dtype).expand(cls_inputs.size(0), cls_inputs.size(1)).contiguous() + 1\r\n",
        "        pos_mask = cls_inputs.eq(self.config.i_pad)\r\n",
        "        positions.masked_fill_(pos_mask, 0)\r\n",
        "\r\n",
        "        outputs = inputs + self.pos_emb(positions)\r\n",
        "        attn_mask = get_attn_pad_mask(cls_inputs, cls_inputs, self.config.i_pad)\r\n",
        "\r\n",
        "        attn_probs = []\r\n",
        "        for layer in self.layers:\r\n",
        "            outputs, attn_prob = layer(outputs, attn_mask)\r\n",
        "            attn_probs.append(attn_prob)\r\n",
        "        return outputs, attn_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUwmpDv7ctDN"
      },
      "source": [
        "class BertEncoder(nn.Module):\r\n",
        "  def __init__(self, bert):\r\n",
        "    super(BertEncoder, self).__init__()\r\n",
        "    self.bert = bert\r\n",
        "\r\n",
        "  def gen_attention_mask(self, token_ids):\r\n",
        "    ## masked attenion, 패딩에 패널티 부여, 학습 x\r\n",
        "    attention_mask = token_ids.ne(1)\r\n",
        "    return attention_mask.float()\r\n",
        "  \r\n",
        "  def forward(self, token_ids):\r\n",
        "    attention_mask = self.gen_attention_mask(token_ids)\r\n",
        "    segment_ids = torch.zeros(token_ids.size())\r\n",
        "\r\n",
        "    output, hidden = self.bert(input_ids = token_ids.long(), token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\r\n",
        "\r\n",
        "    return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyNE0pLqSo_f"
      },
      "source": [
        "class DimensionReducer(nn.Module):\r\n",
        "  def __init__(self, input, output):\r\n",
        "    super(DimensionReducer, self).__init__()\r\n",
        "    self.input = input\r\n",
        "    self.output = output\r\n",
        "    self.fc1 = nn.Linear(self.input,(self.input + self.output)//2)\r\n",
        "    self.fc2 = nn.Linear((self.input + self.output)//2,self.output)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = torch.relu(self.fc1(x))\r\n",
        "    x = torch.relu(self.fc2(x))\r\n",
        "\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX_k46o38Fqm"
      },
      "source": [
        "class BinaryClassifier(nn.Module):\r\n",
        "  def __init__(self, input):\r\n",
        "    super(BinaryClassifier, self).__init__()\r\n",
        "\r\n",
        "    self.input = input\r\n",
        "    self.fc1 = nn.Linear(self.input,128)\r\n",
        "    self.fc2 = nn.Linear(128,64)\r\n",
        "    self.fc3 = nn.Linear(64,32)\r\n",
        "    self.fc4 = nn.Linear(32,1)\r\n",
        "    \r\n",
        "  def forward(self,x):\r\n",
        "    x = torch.relu(self.fc1(x))\r\n",
        "    x = torch.relu(self.fc2(x))\r\n",
        "    x = torch.relu(self.fc3(x))\r\n",
        "    x = torch.sigmoid(self.fc4(x))\r\n",
        "    return x\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thbumRImRlJK"
      },
      "source": [
        "class BERTSummarizer(nn.Module):\r\n",
        "  def __init__(self, config, reducer, transformer, classifier, device):\r\n",
        "    super().__init__()\r\n",
        "    self.config = config\r\n",
        "    self.reducer = reducer\r\n",
        "    self.second_encoder = transformer\r\n",
        "    self.classifier = classifier\r\n",
        "    self.device = device\r\n",
        "\r\n",
        "  def forward(self, new_batch, num):\r\n",
        "\r\n",
        "    # reduce dimension for ram space\r\n",
        "    new_batch = self.reducer(new_batch)\r\n",
        "\r\n",
        "    # second layer  >> [batch, sentence_num, vector]\r\n",
        "    cls_mask = torch.tensor([[1 if i < num[b] else 0 for i in range(new_batch[b].size()[0])] for b in range(len(new_batch))])\r\n",
        "    cls_mask = cls_mask.to(device)\r\n",
        "    new_batch, attn_prob = self.second_encoder(new_batch, cls_mask)\r\n",
        "\r\n",
        "    # classifier  >> [batch, sentence_num, 1]\r\n",
        "    new_batch = self.classifier(new_batch)\r\n",
        "\r\n",
        "    return new_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e93WQoW3RsPt"
      },
      "source": [
        "def embedding(encoder, batch, num):\r\n",
        "  flag = True\r\n",
        "  for i in range(len(batch)):\r\n",
        "    output, hidden = encoder(batch[i][:num[i]].long())\r\n",
        "    pad_num = max_sentence_num - num[i]\r\n",
        "    if pad_num < 0:\r\n",
        "      pad_num = 0\r\n",
        "    article = torch.cat([output[:, 0, :], torch.ones(pad_num,768)], dim=0).unsqueeze(0)\r\n",
        "    if flag:\r\n",
        "      embedded_batch = torch.cat([article, ], dim=0)\r\n",
        "      flag=False\r\n",
        "    else:\r\n",
        "      embedded_batch = torch.cat([embedded_batch, article], dim=0)\r\n",
        "  return embedded_batch\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Nv3Zmc6oHc"
      },
      "source": [
        "# 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si1Y3nHR6HWt"
      },
      "source": [
        "#Config\r\n",
        "class Config(dict):\r\n",
        "  __getattr__ = dict.__getitem__\r\n",
        "  __setattr__ = dict.__setitem__\r\n",
        "\r\n",
        "  @classmethod\r\n",
        "  def load(cls, file):\r\n",
        "    with open(file,'r') as f:\r\n",
        "      config = json.loads(f.read())\r\n",
        "      return Config(config)\r\n",
        "\r\n",
        "config = Config({\r\n",
        "    \"n_enc_vocab\": len(vocab),\r\n",
        "    \"n_dec_vocab\": len(vocab),\r\n",
        "    \"n_enc_seq\":128,\r\n",
        "    \"n_layer\":6,\r\n",
        "    \"d_hidn\":128,\r\n",
        "    \"i_pad\":1,\r\n",
        "    \"d_ff\":1024,\r\n",
        "    \"n_head\":4,\r\n",
        "    \"d_head\":64,\r\n",
        "    \"dropout\":0.1,\r\n",
        "    \"layer_norm_epsilon\":1e-12\r\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFXuXBpTNeL6"
      },
      "source": [
        "# bert 기존 모델만 프리징\r\n",
        "for name, param in bertmodel.named_parameters():\r\n",
        "  param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNr1TiWcXzn_"
      },
      "source": [
        "encoder = BertEncoder(bertmodel)\r\n",
        "# encoder = encoder.to(device)\r\n",
        "reducer = DimensionReducer(768, 128)\r\n",
        "second = SecondEncoder(config=config, n_layer=1)\r\n",
        "classifier = BinaryClassifier(128)\r\n",
        "\r\n",
        "model = BERTSummarizer(config, reducer, second, classifier, device)\r\n",
        "model = model.to(device) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR3FEE6lGda6"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8brz7gIKDOG"
      },
      "source": [
        "learning_rate = 0.01\r\n",
        "num_epochs = 700\r\n",
        "\r\n",
        "# Model , Optimizer, Loss\r\n",
        "\r\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\r\n",
        "loss_fn = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEWyr0Z3ntOq"
      },
      "source": [
        "def save_checkpoint(epoch, model, optimizer, PATH):\n",
        "  torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.to('cpu').state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0-Bkd4Ix5BnI",
        "outputId": "21e85535-600e-45f8-e4ba-2f90e88d83d3"
      },
      "source": [
        "#forward loop\n",
        "losses = []\n",
        "for epoch in range(0, num_epochs):\n",
        "\n",
        "  model.train()\n",
        "  print(epoch)\n",
        "  for batch, num, label in loader:\n",
        "    # bert embedding\n",
        "    new_batch = embedding(encoder, batch, num)\n",
        "\n",
        "    \n",
        "    new_batch = new_batch.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    #calculate output\n",
        "    output = model(new_batch, num)\n",
        "\n",
        "    #calculate loss\n",
        "    loss = loss_fn(output.reshape(-1,1), label.float().reshape(-1,1))\n",
        "    print(loss)\n",
        "\n",
        "\n",
        "    #backprop\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    xm.optimizer_step(optimizer, barrier=True) #TPU를 쓰기 위해 필요한 코드\n",
        "    # optimizer.step()\n",
        "\n",
        "  if epoch%50 == 0:\n",
        "    print(\"epoch {}\\tloss : {}\".format(epoch,loss))\n",
        "   \n",
        "  #  ## checkpoint 저장\n",
        "  # if (epoch+1) % 10 == 0:\n",
        "  if not os.path.exists(\"checkpoints\"):\n",
        "      os.makedirs(\"checkpoints\")\n",
        "  save_checkpoint(epoch+1, model, optimizer, f\"checkpoints/checkpoint_{str(epoch+1)}.pt\")\n",
        "  print(\"checkpoint saved!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "tensor(0.6045, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.6014, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5986, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5954, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5923, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5894, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5865, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5834, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5809, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5778, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5744, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5717, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5688, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5663, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5630, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5600, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5569, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5542, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5512, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5488, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5453, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5423, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5392, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5362, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5337, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5305, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5268, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5239, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5210, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5176, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5144, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5112, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5081, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5050, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.5016, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4985, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4952, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4915, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4878, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4839, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4816, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4781, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4739, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4707, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4671, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4631, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4590, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4554, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4526, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4482, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4442, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4400, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4362, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4312, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4279, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4242, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4189, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4144, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4106, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4064, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.4015, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3976, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3917, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3877, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3840, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3788, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3745, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3700, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3664, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3611, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3562, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3515, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3484, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3424, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3377, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3341, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3281, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3240, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3192, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3151, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3113, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3061, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.3034, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2979, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2942, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2892, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2843, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2806, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2768, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2738, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2687, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2638, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2597, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2573, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2540, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2529, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2460, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2436, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2407, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2375, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2339, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2312, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2291, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2258, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2234, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2199, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2186, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2158, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2134, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2098, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2089, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2072, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2052, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.2016, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1999, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1991, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1976, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1948, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1937, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1915, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1909, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1901, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1886, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1867, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1854, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1829, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1825, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1825, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1808, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1787, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1797, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1773, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1770, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1757, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1760, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1737, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1734, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1732, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1715, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1724, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1712, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1696, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1691, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1698, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1684, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1681, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1668, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1664, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1660, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1642, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1640, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1634, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1657, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1622, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1631, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1618, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1627, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1604, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1608, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1598, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1601, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1591, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1582, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1570, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1588, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1578, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1593, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1558, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1569, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1574, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1555, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1562, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1548, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1545, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1548, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1559, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1542, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1536, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1530, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1543, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1539, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1526, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1527, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1521, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1512, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1518, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1522, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1503, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1502, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1501, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1493, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1501, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1489, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1488, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1480, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1499, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1465, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1485, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1483, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1467, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1457, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1451, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1473, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1471, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1467, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1452, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1460, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1458, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1458, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1459, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1462, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1462, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1428, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1424, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1436, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1419, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1431, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1428, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1407, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1424, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1413, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1409, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1407, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1396, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1406, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1421, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1385, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1390, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1388, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1386, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1393, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1381, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1386, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1388, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1395, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1361, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1388, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1373, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1369, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1380, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1378, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1373, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1368, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1368, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1354, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1337, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1350, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1356, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1387, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1363, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1348, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1344, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1369, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1359, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1318, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1334, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1345, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1322, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1325, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1348, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1317, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1356, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1317, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1302, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1293, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1301, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1333, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1297, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1304, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1304, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1325, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1300, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1298, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1266, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1303, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1298, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1298, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1305, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1317, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1317, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1285, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1296, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1353, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1284, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1281, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1288, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1292, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1310, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1273, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1260, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1257, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1270, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1265, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1237, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1249, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1235, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1284, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1274, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1282, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1271, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1290, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1268, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1234, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1278, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1224, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1248, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1296, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1241, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1251, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1283, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1218, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1236, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1247, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1235, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1237, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1249, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1212, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1216, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1302, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1253, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1219, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1208, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1232, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1229, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1195, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1197, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1259, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1242, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1223, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1249, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1217, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1185, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1222, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1220, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1194, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1224, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1181, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1193, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1229, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1225, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1229, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1210, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1193, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1208, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1217, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1211, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1230, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1211, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1184, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1160, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1207, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1202, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1205, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1192, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1195, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1164, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1207, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1190, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1207, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1209, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1193, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1187, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1197, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1221, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1170, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1201, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1178, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1201, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1166, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1173, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1212, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1168, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1215, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1149, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1209, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1183, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1178, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1178, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1173, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1198, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1185, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1190, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1161, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1204, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1154, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1168, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1195, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1235, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1143, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1191, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1220, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1155, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1139, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1167, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1159, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1170, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1216, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1176, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1172, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1167, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1160, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1158, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1150, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1139, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1152, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1161, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1193, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1171, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1173, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1206, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1188, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1186, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1187, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1197, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1181, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1155, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1148, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1156, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1147, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1153, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1151, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1150, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1191, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1125, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1150, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1150, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1166, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1139, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1190, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1154, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1137, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1153, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1174, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1161, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1129, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1145, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1130, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1177, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1150, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1205, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1130, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1134, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1135, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1158, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1186, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1125, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1101, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1181, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1082, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1127, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1174, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1105, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1142, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1107, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1162, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1188, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1137, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1114, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1171, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1148, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1162, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1137, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1136, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1112, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1201, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1083, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1128, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1113, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1122, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1160, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1133, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1145, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1132, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1121, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1130, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1112, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1177, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1167, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1088, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1156, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1169, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1182, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1148, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1169, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1157, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1168, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1156, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1129, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1133, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1137, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1146, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1127, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1130, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1144, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1153, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1137, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1215, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1119, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1156, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1163, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1110, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1096, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1108, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1131, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1160, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1136, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1124, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1187, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1130, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1120, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1110, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1103, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1129, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1138, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1186, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1077, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1141, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1175, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1101, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1151, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1106, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1148, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1116, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1178, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1142, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1108, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1108, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1103, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1145, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1058, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1135, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1137, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1130, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1155, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1093, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1130, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1144, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1148, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1107, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1107, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1114, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1125, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1121, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1187, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1115, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1155, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1191, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1148, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1122, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1114, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1080, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1105, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1121, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1141, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1143, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1118, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1102, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1125, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1101, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1114, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1133, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1110, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1095, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1122, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1137, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1147, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1128, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1153, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1091, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1120, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1103, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1137, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1134, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1145, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1105, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1089, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1095, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1127, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1082, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1180, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1153, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1118, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1144, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1181, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1128, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1069, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1153, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1139, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1163, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1111, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1110, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1116, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1115, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1143, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1089, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1107, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1106, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1164, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1085, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1137, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1131, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1084, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1138, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1123, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1142, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1135, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1129, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1121, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1173, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1089, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1089, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1075, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1126, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1143, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1092, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1116, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1079, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1109, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1114, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1094, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1118, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1073, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1120, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1158, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1142, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1132, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1143, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1146, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1103, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1128, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1148, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1111, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1125, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1196, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1115, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1083, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1148, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1036, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1093, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1105, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1130, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1079, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1087, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1091, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1112, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1123, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1096, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1118, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1063, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1059, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1089, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1095, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1125, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1053, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1119, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1168, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1107, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1095, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1120, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1117, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1075, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1072, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1138, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1095, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1056, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1058, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1102, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1140, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor(0.1079, device='xla:1', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 0\tloss : 0.10793977975845337\n",
            "checkpoint saved!\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-8ee2116f5834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#calculate output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mit\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmodified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mArguments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mpersistent\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-109-c508fe2bd59e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, new_batch, num)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# reduce dimension for ram space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mnew_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# second layer  >> [batch, sentence_num, vector]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mit\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmodified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mArguments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mpersistent\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-107-3e0ad0969ec9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mit\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmodified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mArguments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mpersistent\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         )\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBilinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m     \u001b[0;31m# type: (Tensor, List[int], Optional[Tensor], Optional[Tensor], float) -> Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     r\"\"\"Applies Layer Normalization for last certain number of dimensions.\n\u001b[0;32m-> 1692\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \"\"\"\n",
            "\u001b[0;31mRuntimeError\u001b[0m: torch_xla/csrc/aten_xla_bridge.cpp:69 : Check failed: xtensor \n*** Begin stack trace ***\n\ttensorflow::CurrentStackTrace()\n\ttorch_xla::bridge::GetXlaTensor(at::Tensor const&)\n\ttorch_xla::AtenXlaType::mm(at::Tensor const&, at::Tensor const&)\n\tc10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&> >, at::Tensor (at::Tensor const&, at::Tensor const&)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&)\n\tat::Tensor c10::Dispatcher::callWithDispatchKey<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, c10::DispatchKey, at::Tensor const&, at::Tensor const&) const\n\tat::mm(at::Tensor const&, at::Tensor const&)\n\t\n\t\n\tat::Tensor c10::Dispatcher::callWithDispatchKey<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, c10::DispatchKey, at::Tensor const&, at::Tensor const&) const\n\tat::Tensor::mm(at::Tensor const&) const\n\t\n\tat::native::matmul(at::Tensor const&, at::Tensor const&)\n\t\n\t\n\tat::Tensor c10::Dispatcher::callWithDispatchKey<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, c10::DispatchKey, at::Tensor const&, at::Tensor const&) const\n\tat::Tensor::matmul(at::Tensor const&) const\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_FastCallDict\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_FastCallDict\n\t\n\t\n\t_PyObject_FastCallKeywords\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_FastCallDict\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_FastCallDict\n\t\n\t\n\t_PyObject_FastCallKeywords\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_FastCallDict\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_FastCallDict\n\t\n\t\n\t_PyObject_FastCallKeywords\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_FastCallDict\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_FastCallDict\n\t_PyObject_FastCallDict\n\t\n\t_PyObject_FastCallKeywords\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_FastCallDict\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n*** End stack trace ***\nInput tensor is not an XLA tensor: torch.FloatTensor"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiD_p0EMVgrB"
      },
      "source": [
        "# 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgbMyEZOvQ_4"
      },
      "source": [
        "# 모델 불러오기\n",
        "save_path = '/content/gdrive/My Drive/context_summarize/checkpoints/checkpoint_1.tar'\n",
        "checkpoint = torch.load(save_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# input을 이제 dataloader에 넣어야하는데 이부분에서 df[:1] 대신 사용자의 데이터를 넣어야할 것 같은데,,, 음.. 이게 가능하려나 어렵네,\n",
        "# 내가 생각하기에 input을 하나만 넣으면 맞춰서 돌아가는 구조? --> 정 안되면 로컬 dataloader나 saver를 사용하는 것도 좋을 듯한데\n",
        "train_dataset = ArticleLineDataset(df[:1], \"article_original\", \"extractive\", tok, max_sentence_num, max_word_num, True, False)\n",
        "loader = MyDataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "# 모델 불러서 output\n",
        "for batch, num, label in loader:\n",
        "    # bert embedding\n",
        "    new_batch = embedding(encoder, batch, num)\n",
        "\n",
        "    new_batch = new_batch.to(device)\n",
        "    label = label.to(device)\n",
        "    #calculate output\n",
        "    output = model(new_batch, num)\n",
        "\n",
        "print(output.shape)\n",
        "# outputsize가 \n",
        "test = output[0].squeeze(1)\n",
        "print(test.shape)\n",
        "\n",
        "result = []\n",
        "for i, out in enumerate(test):\n",
        "  result.append([i,out])\n",
        "\n",
        "result.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "top_three_sentences = result[0:3]\n",
        "top_three_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhZqJQhW-PCr"
      },
      "source": [
        "# 모델 불러오기\r\n",
        "save_path = '/content/gdrive/My Drive/context_summarize/'\r\n",
        "checkpoint = torch.load(save_path)\r\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\r\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}